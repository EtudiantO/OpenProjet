import os
import csv
from bs4 import BeautifulSoup
import requests
import shutil
import urllib.request


def get_all_categories():
    homepage="https://books.toscrape.com"

    reponse = requests.get(homepage)
    soup = BeautifulSoup(reponse.content.decode("utf-8", "ignore"), features="html.parser")
    categories=soup.find('div',{'class':'side_categories'}).findAll('a')
    #print(categories)
    listcat=[]
    for category in categories:
        dic={}
        dic[category.text.strip()]=category['href']

       # for x,y in dic.items():
          #  print(x,":",y)
        listcat+=[dic]
   # print(listcat)

    with open(os.path.abspath("C:/Users/Idrissa TOUNKARA/PycharmProjects/testProjet/Fichiers/fille.csv"), 'w', newline='') as fs:
             writer = csv.writer(fs, delimiter=';', lineterminator='\r\n')
             #print(elst)
             writer.writerows([(i,) for i in listcat])

    with open(os.path.abspath("C:/Users/Idrissa TOUNKARA/PycharmProjects/testProjet/Fichiers/fille.csv"), newline='', encoding='utf-8') as fs:
                reader = csv.reader(fs)
              #  for row in reader:
                  #  print(row)

get_all_categories()

def get_all_books():

   homepage = "https://books.toscrape.com/"
   reponse = requests.get(homepage)
   soup = BeautifulSoup(reponse.content.decode("utf-8", "ignore"), features="html.parser")

   categories = soup.find('div', {'class': 'side_categories'}).findAll('a')
   # print(categories)
   listcat = []
   for category in categories:
       dic = {}
       dic[category.text.strip()] =homepage+category['href']
       #print(dic)
       for i,y in dic.items():

           # b="http://books.toscrape.com/"+y
           # print(y)

            reponse1 = requests.get(y)
            soup = BeautifulSoup(reponse1.content.decode("utf-8", "ignore"), features="html.parser")

            if reponse.ok:

                list_book = soup.find_all("img")  # afficher tout les liens images de livres
                list = []
                for image in list_book:
                    list.append(image['src'])

                    with open(os.path.abspath("C:/Users/Idrissa TOUNKARA/PycharmProjects/testProjet/LesCategories/"+i+".csv"), 'w',   newline='') as ffichier:
                        writer = csv.writer(ffichier, delimiter=';', lineterminator='\r\n')
                        writer.writerows([(i,) for i in list])
                nbr_page=soup.find("li",{"class": "current"})
                if nbr_page:
                  for k in range(2,int(nbr_page.text.strip()[-1])+ 1):
                      toto=y.split("/")
                      toto[-1]="page-"+str(k)+".html"
                      url="/".join(toto)
                      kb =requests.get(url)
                      soup = BeautifulSoup(kb.content.decode("utf-8", "ignore"), features="html.parser")
                      list_book = soup.find_all("img")
                      for image in list_book:
                          list.append(image['src'])


get_all_books()

def get_product_page(url_book):

    reponse = requests.get(url_book)
    soup3 = BeautifulSoup(reponse.content.decode("utf-8", "ignore"), features="html.parser")

    if reponse.ok:
        ret={}
        ret['titre']=soup3.find('h1').text
        ret['description'] = soup3.find('meta', {'name': 'description'})['content']
        information = soup3.find_all('td')
        ret['upc_livre'] = information[0].text
        ret['product_type']=information[1].text
        ret['prix_ht'] = information[2].text
        ret['prix_ttc']=information[3].text
        ret['tax']=information[4].text
        ret['stock']=information[5].text
        for x,y in ret.items():
           print(x,":",y)

get_product_page("https://books.toscrape.com/catalogue/its-only-the-himalayas_981/index.html")

def get_book_categorie():
    homepage = "https://books.toscrape.com/"
    reponse = requests.get(homepage)
    soup = BeautifulSoup(reponse.content.decode("utf-8", "ignore"), features="html.parser")
    categories = soup.find('div', {'class': 'side_categories'}).findAll('a')

    for category in categories[1:]:


        i=category.text.strip()
        y=homepage+category['href']

        reponse=requests.get(y)
        soup=BeautifulSoup(reponse.content.decode("utf-8", "ignore"),features="html.parser")
        books=soup.find('section').findAll('a')
        l=[]
        for b in books:
            #print(b)
            bb=b['href']
            t=(homepage+bb).replace("../","")
            l.append(get_product_page(t))

            if b['href'] not in l:
                l.append(b['href'])
               # print(l)
        with open(os.path.abspath("C:/Users/Idrissa TOUNKARA/PycharmProjects/testProjet/Books/"+i+".csv"), 'w',newline='') as ffichier:
            entetes = [
                "product_page_url",
                "universal_product_code",
                "title",
                "price_including_tax",
                "price_excluding_tax",
                "number_available",
                "product_description",
                "category",
                "reviews_rating",
                "image_url",
                "image_name",
            ]
            ligne_entetes = ";".join(entetes) + "\n"
            ffichier.write(ligne_entetes)
            for ligne in l:

                ligne_to_add=[ #ret['upc_livre'],
                               ligne['product_type'],
                               ligne['prix_ht'],
                               ligne['prix_ttc'],
                               ligne['tax'],
                               ligne['stock']

                ]
                ligne_to_add=";".join(ligne_to_add) + "\n"
                ffichier.write(ligne_to_add)
            writer = csv.writer(ffichier, delimiter=';', lineterminator='\r\n')
            writer.writerows([(i,) for i in l])
get_book_categorie()


def imageBook():
    homepage = "https://books.toscrape.com/"
    reponse = requests.get(homepage)
    soup = BeautifulSoup(reponse.content.decode("utf-8", "ignore"), features="html.parser")
    categories = soup.find('div', {'class': 'side_categories'}).findAll('a')
    for category in categories:
        y=category['href']
        reponse=requests.get(homepage+y)
        soup= BeautifulSoup(reponse.content.decode("utf-8", "ignore"), features="html.parser")
        imgB=soup.find_all("img")
        #l=[]
        for mb in imgB:
            m=mb['src']
            img=(homepage+m).replace("../","")
            name=mb['alt']
           # print(img)

            urllib.request.urlretrieve(img,("LesImages/"+(name).replace(".","")+".jpg").replace(":","").replace("*","").replace("?","").replace('''"''',""))

imageBook()





