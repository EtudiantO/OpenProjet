import os
import csv
from bs4 import BeautifulSoup
import requests
import shutil
import urllib.request


def get_all_categories():
    homepage="https://books.toscrape.com"

    reponse = requests.get(homepage)
    soup = BeautifulSoup(reponse.content.decode("utf-8", "ignore"), features="html.parser")
    categories=soup.find('div',{'class':'side_categories'}).findAll('a')
    #print(categories)
    listcat=[]
    for category in categories:
        dic={}
        dic[category.text.strip()]=category['href']

       # for x,y in dic.items():
          #  print(x,":",y)
        listcat+=[dic]
   # print(listcat)

    with open(os.path.abspath("C:/Users/Idrissa TOUNKARA/PycharmProjects/testProjet/Fichiers/fille.csv"), 'w', newline='') as fs:
             writer = csv.writer(fs, delimiter=';', lineterminator='\r\n')
             #print(elst)
             writer.writerows([(i,) for i in listcat])

   # with open(os.path.abspath("C:/Users/Idrissa TOUNKARA/PycharmProjects/testProjet/Fichiers/fille.csv"), newline='', encoding='utf-8') as fs:
                #reader = csv.reader(fs)
              #  for row in reader:
                  #  print(row)

get_all_categories()

def get_all_books():

   homepage = "https://books.toscrape.com/"
   reponse = requests.get(homepage)
   soup = BeautifulSoup(reponse.content.decode("utf-8", "ignore"), features="html.parser")

   categories = soup.find('div', {'class': 'side_categories'}).findAll('a')
   # print(categories)
   listcat = []
   for category in categories:
       dic = {}
       dic[category.text.strip()] =homepage+category['href']
       #print(dic)
       for i,y in dic.items():

           # b="http://books.toscrape.com/"+y
           # print(y)

            reponse1 = requests.get(y)
            soup = BeautifulSoup(reponse1.content.decode("utf-8", "ignore"), features="html.parser")

            if reponse.ok:

                list_book = soup.find_all("img")  # afficher tout les liens images de livres
                list = []
                for image in list_book:
                    list.append(image['src'])

                    with open(os.path.abspath("C:/Users/Idrissa TOUNKARA/PycharmProjects/testProjet/LesCategories/"+i+".csv"), 'w',   newline='') as ffichier:
                        writer = csv.writer(ffichier, delimiter=';', lineterminator='\r\n')
                        writer.writerows([(i,) for i in list])
                nbr_page=soup.find("li",{"class": "current"})
                if nbr_page:
                  for k in range(2,int(nbr_page.text.strip()[-1])+ 1):
                      toto=y.split("/")
                      toto[-1]="page-"+str(k)+".html"
                      url="/".join(toto)
                     # print(url)
                      kb =requests.get(url)
                      soup = BeautifulSoup(kb.content.decode("utf-8", "ignore"), features="html.parser")
                      list_book = soup.find_all("img")
                      for image in list_book:
                          list.append(image['src'])


get_all_books()

"""def get_product_page(url_book):

    reponse = requests.get(url_book)
    soup3 = BeautifulSoup(reponse.content.decode("utf-8", "ignore"), features="html.parser")

    if reponse.ok:
        ret={}
        ret['titre']=soup3.find('h1').text
        ret['description'] = soup3.find('meta', {'name': 'description'})['content']
        information = soup3.find_all('td')
        ret['upc_livre'] = information[0].text
        ret['product_type']=information[1].text
        ret['prix_ht'] = information[2].text
        ret['prix_ttc']=information[3].text
        ret['tax']=information[4].text
        ret['stock']=information[5].text
        for x,y in ret.items():
           print(x,":",y)

        return ret

get_product_page("https://books.toscrape.com/catalogue/its-only-the-himalayas_981/index.html")"""

def get_book_categorie():
    homepage = "https://books.toscrape.com/"
    reponse = requests.get(homepage)
    soup = BeautifulSoup(reponse.content.decode("utf-8", "ignore"), features="html.parser")
    categories = soup.find('div', {'class': 'side_categories'}).findAll('a')

    for category in categories[1:]:


        i=category.text.strip()
        y=homepage+category['href']

        reponse=requests.get(y)
        soup=BeautifulSoup(reponse.content.decode("utf-8", "ignore"),features="html.parser")
        url_h3 = soup.findAll('h3')

        print(url_h3)
        l=[]
        url_image = []
        for h3 in url_h3:
            url_image.append(h3.find('a'))
        
        with open(os.path.abspath("C:/Users/Idrissa TOUNKARA/PycharmProjects/testProjet/Books/" + i + ".csv"), 'w', newline='') as fichier:
            entetes = [
                "product_page_url",
                "universal_product_code",
                "title",
                "price_including_tax",
                "price_excluding_tax",
                "number_available",
                "product_description",
                "category",
                "reviews_rating",
                "image_url",
                "image_name",
            ]
            ligne_entetes = ";".join(entetes) + "\n"
            fichier.write(ligne_entetes)
     

        for b in url_image:
            
                #print(b)
                bb=b['href']

                t = (homepage+"catalogue/"+bb).replace("../", "")
                #print(t)
                reponse = requests.get(t)
                soup3 = BeautifulSoup(reponse.content.decode("utf-8", "ignore"), features="html.parser")
                infos = soup3.find_all('td')
                titre=soup3.find('h1').text
                description=soup3.find('meta', {'name': 'description'})['content']
                reviews_rating=soup3.find('p',{'class':'star-rating'})['class'][1]
                image_url=soup3.find('img')['src']
                image_name=soup3.find('img')['alt']


                if b['href'] not in l:
                    l.append(b['href'])
                   # print(l)

                try:
                    l = [
                            t,
                            infos[0].text,
                            titre,
                            infos[3].text,
                            infos[2].text,
                            infos[5].text,
                            description.replace("\n",""),
                            i,
                            reviews_rating,
                            image_url,
                            image_name


                           ]
                except:
                    pass
              



                with open(os.path.abspath("C:/Users/Idrissa TOUNKARA/PycharmProjects/testProjet/Books/" + i + ".csv"), 'a', encoding="utf-8", newline='') as fichier:
                     mylist = ";".join(l) + "\n"
                     fichier.write(mylist)

        nbr_page = soup.find("li", {"class": "current"})
        if nbr_page:
                for k in range(2, int(nbr_page.text.strip()[-1]) + 1):
                    toto = y.split("/")
                    toto[-1] = "page-" + str(k) + ".html"
                    url = "/".join(toto)
                           # print(url)


                    kb = requests.get(url)
                    soup3 = BeautifulSoup(kb.content.decode("utf-8", "ignore"), features="html.parser")
                    url_h3= soup3.findAll('h3')

                    print(url_h3)
                    cpt=0
                    url_image=[]
                    for h3 in url_h3:
                        url_image.append(h3.find('a'))
                    for u in url_image:

                          



                                ul=(homepage+"catalogue/"+u['href']).replace("../","")
                                #print(ul)
                                reponse=requests.get(ul)
                                soup3=BeautifulSoup(reponse.content.decode("utf-8", "ignore"), features="html.parser")
                                infos=soup3.find_all('td')
                                #f=[infos[0].text,infos[2].text,infos[3].text,infos[5].text]
                                #print(f)
                                description = soup3.find('meta', {'name': 'description'})['content']
                                reviews_rating = soup3.find('p', {'class': 'star-rating'})['class'][1]
                                image_url = soup3.find('img')['src']
                                image_name = soup3.find('img')['alt']
                                #if u['href'] not in l:
                                   # l.append(u['href'])
                                try:
                                    l = [
                                        t,
                                        infos[0].text,
                                        titre,
                                        infos[3].text,
                                        infos[2].text,
                                        infos[5].text,
                                        description.replace("\n", ""),
                                        i,
                                        reviews_rating,
                                        image_url,
                                        image_name

                                        ]
                                except:
                                    pass
                               
                  
                                with open(os.path.abspath("C:/Users/Idrissa TOUNKARA/PycharmProjects/testProjet/Books/"+i+".csv"), 'a',encoding="utf-8", newline='') as fichier:
                                                mylist = ";".join(l) + "\n"
                                                fichier.write(mylist)


get_book_categorie()


def imageBook():
    homepage = "https://books.toscrape.com/"
    reponse = requests.get(homepage)
    soup = BeautifulSoup(reponse.content.decode("utf-8", "ignore"), features="html.parser")
    categories = soup.find('div', {'class': 'side_categories'}).findAll('a')
    for category in categories:
        y=category['href']
        reponse=requests.get(homepage+y)
        soup= BeautifulSoup(reponse.content.decode("utf-8", "ignore"), features="html.parser")
        imgB=soup.find_all("img")
        #l=[]
        for mb in imgB:
            m=mb['src']
            img=(homepage+m).replace("../","")
            name=mb['alt']
           # print(img)

            urllib.request.urlretrieve(img,("LesImages/"+(name).replace(".","")+".jpg").replace(":","").replace("*","").replace("?","").replace('''"''',""))

imageBook()





